{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajitakarkee/Natural-Language-Processing-NLP-/blob/main/NLP_Lab_1%5BTextCleaningBasics%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning Basics**"
      ],
      "metadata": {
        "id": "A6ndVuGRkL8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) HTML Tags removal:\n"
      ],
      "metadata": {
        "id": "MGkMAkiFkx7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#Example:\n",
        "\n",
        "\"<p><b>Hello!!!</b> My name is <i>AJITA KARKI</i> and I am currently <u>STUDYING</u> <span>Bachelor in Computer Engineering</span> at <a href=\"#\">Pokhara University</a> and I am in <strong>7th semester</strong>. In my <strong>Natural Language Processing LAB</strong>, I learned about <em>TEXT CLEANING BASICS</em> such as removing <div>HTML tags</div>, handling <FONT>Upper & Lower CASE</FONT> inconsistency, deleting <mark>punctuations!!!</mark>, removing <small>STOPWORDS</small>, and converting words into their <code>ROOT FORM</code> using lemmatization and stemming.</p>\n",
        "\"\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ynClrhc6k6k7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "W2EV4YLBj7FX",
        "outputId": "2f8aaa59-4e67-49ce-e6ef-13591a9f007a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello!!! My name is AJITA KARKI and I am currently STUDYING Bachelor in Computer Engineering at Pokhara University and I am in 7th semester. In my Natural Language Processing LAB, I learned about TEXT CLEANING BASICS such as removing HTML tags, handling Upper & Lower CASE inconsistency, deleting punctuations!!!, removing STOPWORDS, and converting words into their ROOT FORM using lemmatization and stemming.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import re\n",
        "text_data = '''<p><b>Hello!!!</b> My name is <i>AJITA KARKI</i> and I am currently <u>STUDYING</u> <span>Bachelor in Computer Engineering</span> at <a href=\"#\">Pokhara University</a> and I am in <strong>7th semester</strong>. In my <strong>Natural Language Processing LAB</strong>, I learned about <em>TEXT CLEANING BASICS</em> such as removing <div>HTML tags</div>, handling <FONT>Upper & Lower CASE</FONT> inconsistency, deleting <mark>punctuations!!!</mark>, removing <small>STOPWORDS</small>, and converting words into their <code>ROOT FORM</code> using lemmatization and stemming.</p>'''\n",
        "html_pattern = re.compile('<.*?>')\n",
        "text_data = re.sub(html_pattern, '', text_data)\n",
        "text_data = text_data.replace('\\n', ' ')\n",
        "text_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Upper and lower case inconsistency:**\n"
      ],
      "metadata": {
        "id": "_ZPW9LfmmkKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = text_data.lower()\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SNDXo8M-kw4i",
        "outputId": "bb11a028-a2d2-46f1-8955-7b2e6a0fd7e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello!!! my name is ajita karki and i am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester. in my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations!!!, removing stopwords, and converting words into their root form using lemmatization and stemming.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Remove Punctuations:**"
      ],
      "metadata": {
        "id": "Kr1zt803m2uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = re.sub(r'[^\\w\\s]', '', text_data)\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9tsVCyKumx_7",
        "outputId": "107b3ad4-1c3a-40b5-cb4e-a884edabc1e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello name ajita karki and currently studying bachelor computer engineering pokhara university and 7th semester natural language processing lab learned about text cleaning basics such removing html tags handling upper lower case inconsistency deleting punctuations removing stopwords and converting words into their root form using lemmatization and stemming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Remove words having length less than or equal to 2:**"
      ],
      "metadata": {
        "id": "ddMb-baonNPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = ' '.join(word for word in text_data.split() if len(word)>2)\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bWxgD6DxnQ7Y",
        "outputId": "5e99a375-15c7-4390-8748-885a5efaf456"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello name ajita karki and currently studying bachelor computer engineering pokhara university and 7th semester natural language processing lab learned about text cleaning basics such removing html tags handling upper lower case inconsistency deleting punctuations removing stopwords and converting words into their root form using lemmatization and stemming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "fQgey-Dgnezv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Sentence Tokenizer:**"
      ],
      "metadata": {
        "id": "WLf2au-2nove"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9EmxJhVn9JO",
        "outputId": "872a8f0d-18ed-4188-e4a6-1bfef312d5f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq32av0fCwhI",
        "outputId": "2dd0a4cf-9f21-491d-cfde-8c2f3fe48cce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text_data = 'Hello, my name is ajita karki and I am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester. In my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations, removing stopwords, and converting words into their root form using lemmatization and stemming.'\n",
        "sent_tokenize(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4sih2V9nzet",
        "outputId": "7d775876-c5f0-42e4-865d-2966a4c48396"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, my name is ajita karki and I am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester.',\n",
              " 'In my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations, removing stopwords, and converting words into their root form using lemmatization and stemming.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Word Tokenizer:**"
      ],
      "metadata": {
        "id": "A6AI2y5fnyiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text_data = 'Hello, my name is ajita karki and I am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester. In my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations, removing stopwords, and converting words into their root form using lemmatization and stemming.'\n",
        "tokenized_text= word_tokenize(text_data)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10wJiW4kodsz",
        "outputId": "fcd460eb-5af8-4f95-945d-f47e5fbfef24"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'my', 'name', 'is', 'ajita', 'karki', 'and', 'I', 'am', 'currently', 'studying', 'bachelor', 'in', 'computer', 'engineering', 'at', 'pokhara', 'university', 'and', 'i', 'am', 'in', '7th', 'semester', '.', 'In', 'my', 'natural', 'language', 'processing', 'lab', ',', 'i', 'learned', 'about', 'text', 'cleaning', 'basics', 'such', 'as', 'removing', 'html', 'tags', ',', 'handling', 'upper', '&', 'lower', 'case', 'inconsistency', ',', 'deleting', 'punctuations', ',', 'removing', 'stopwords', ',', 'and', 'converting', 'words', 'into', 'their', 'root', 'form', 'using', 'lemmatization', 'and', 'stemming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) WhitespaceTokenizer:**"
      ],
      "metadata": {
        "id": "bP2bXviNnEwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "text_data = 'Hello, my name is ajita karki and I am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester. In my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations, removing stopwords, and converting words into their root form using lemmatization and stemming.'\n",
        "print(WhitespaceTokenizer().tokenize(text_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAjrS7eaopKK",
        "outputId": "fff09e85-1ad8-4daf-8379-d7f9e4405da9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', 'my', 'name', 'is', 'ajita', 'karki', 'and', 'I', 'am', 'currently', 'studying', 'bachelor', 'in', 'computer', 'engineering', 'at', 'pokhara', 'university', 'and', 'i', 'am', 'in', '7th', 'semester.', 'In', 'my', 'natural', 'language', 'processing', 'lab,', 'i', 'learned', 'about', 'text', 'cleaning', 'basics', 'such', 'as', 'removing', 'html', 'tags,', 'handling', 'upper', '&', 'lower', 'case', 'inconsistency,', 'deleting', 'punctuations,', 'removing', 'stopwords,', 'and', 'converting', 'words', 'into', 'their', 'root', 'form', 'using', 'lemmatization', 'and', 'stemming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop word Removal**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6m09ac0No1eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVuiwz1JpSPT",
        "outputId": "e361104c-8c9c-4196-8599-3fb034153fcb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDu_Kjc_pin-",
        "outputId": "b4ba7399-0758-49c8-f1ee-06319e2d9f19"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0L9PgO_pl6N",
        "outputId": "72384da8-5040-4b33-8535-f417b9d6b6d5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = 'Hello, my name is ajita karki and I am currently studying bachelor in computer engineering at pokhara university and i am in 7th semester. In my natural language processing lab, i learned about text cleaning basics such as removing html tags, handling upper & lower case inconsistency, deleting punctuations, removing stopwords, and converting words into their root form using lemmatization and stemming.'\n",
        "text_data = re.sub(r'[^\\w\\s]', '', text_data) #punctuation removal\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_text = word_tokenize(text_data)\n",
        "tokenized_text = word_tokenize(text_data)\n",
        "print(tokenized_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "f-9e0p92p9GP",
        "outputId": "4f46c2a6-e971-4e1d-a226-595e94c0a3ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'my', 'name', 'is', 'ajita', 'karki', 'and', 'I', 'am', 'currently', 'studying', 'bachelor', 'in', 'computer', 'engineering', 'at', 'pokhara', 'university', 'and', 'i', 'am', 'in', '7th', 'semester', 'In', 'my', 'natural', 'language', 'processing', 'lab', 'i', 'learned', 'about', 'text', 'cleaning', 'basics', 'such', 'as', 'removing', 'html', 'tags', 'handling', 'upper', 'lower', 'case', 'inconsistency', 'deleting', 'punctuations', 'removing', 'stopwords', 'and', 'converting', 'words', 'into', 'their', 'root', 'form', 'using', 'lemmatization', 'and', 'stemming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words_list = [word for word in tokenized_text if word not in stopwords]\n",
        "print(removed_stop_words_list)"
      ],
      "metadata": {
        "id": "4quenzz1tU5M",
        "outputId": "27bb7fc8-8475-4fec-f62d-679300f0a9ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'name', 'ajita', 'karki', 'I', 'currently', 'studying', 'bachelor', 'computer', 'engineering', 'pokhara', 'university', '7th', 'semester', 'In', 'natural', 'language', 'processing', 'lab', 'learned', 'text', 'cleaning', 'basics', 'removing', 'html', 'tags', 'handling', 'upper', 'lower', 'case', 'inconsistency', 'deleting', 'punctuations', 'removing', 'stopwords', 'converting', 'words', 'root', 'form', 'using', 'lemmatization', 'stemming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.append('fmcg')\n",
        "len(stopwords)\n"
      ],
      "metadata": {
        "id": "9LfjB90It7K8",
        "outputId": "60a7c1c2-3e97-41bb-8a73-950716cb55cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words_list = [word for word in tokenized_text if word not in stopwords]\n",
        "print(removed_stop_words_list)\n"
      ],
      "metadata": {
        "id": "ZWFyy9ROuEiv",
        "outputId": "033195a9-a57c-412c-811b-4020316086e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'name', 'ajita', 'karki', 'I', 'currently', 'studying', 'bachelor', 'computer', 'engineering', 'pokhara', 'university', '7th', 'semester', 'In', 'natural', 'language', 'processing', 'lab', 'learned', 'text', 'cleaning', 'basics', 'removing', 'html', 'tags', 'handling', 'upper', 'lower', 'case', 'inconsistency', 'deleting', 'punctuations', 'removing', 'stopwords', 'converting', 'words', 'root', 'form', 'using', 'lemmatization', 'stemming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming & Lemmatization**\n"
      ],
      "metadata": {
        "id": "Jh50tdOgu8oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "print(porter.stem(\"running\"))\n",
        "print(porter.stem(\"runner\"))\n",
        "print(porter.stem(\"runs\"))\n",
        "print(porter.stem(\"ran\"))\n"
      ],
      "metadata": {
        "id": "mAQ2nUBTvZPk",
        "outputId": "f99849a0-27f8-42b3-9f14-107dd1936c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "runner\n",
            "run\n",
            "ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter.stem(\"singing\"))\n",
        "print(porter.stem(\"sings\"))\n",
        "print(porter.stem(\"sang\"))\n",
        "print(porter.stem(\"song\"))\n"
      ],
      "metadata": {
        "id": "AcG-LGA7uNJl",
        "outputId": "02962167-db9d-402c-a25f-7dff32e325a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sing\n",
            "sing\n",
            "sang\n",
            "song\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Lemmatization:**"
      ],
      "metadata": {
        "id": "ZrxZlRhbzt81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "x0-r4T2f07ZT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "print(wordnet_lemmatizer.lemmatize(\"running\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"runs\")) # Lemmatizer is able to convert it to \"go\"\n",
        "print(wordnet_lemmatizer.lemmatize(\"ran\"))\n",
        "print(porter.stem(\"played\")) # Stemming is unable to normalize the word \"goes\" properly"
      ],
      "metadata": {
        "id": "Hp3XQ-8B1HVs",
        "outputId": "d8599645-3378-41ff-a063-9b4a081530aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n",
            "run\n",
            "ran\n",
            "play\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Verbs\n",
        "print(wordnet_lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"jumps\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"ate\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"play\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"teaches\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"teaching\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"taught\", pos=\"v\"))\n",
        "\n",
        "# Nouns\n",
        "print(wordnet_lemmatizer.lemmatize(\"cats\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"cats\", pos=\"n\"))\n"
      ],
      "metadata": {
        "id": "2ar6qcWU1ryy",
        "outputId": "96c9f612-672c-420a-9df5-2c0e90e854f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "jump\n",
            "eat\n",
            "play\n",
            "teach\n",
            "teach\n",
            "teach\n",
            "cat\n",
            "cat\n"
          ]
        }
      ]
    }
  ]
}